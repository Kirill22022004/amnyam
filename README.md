# Кудряшов Кирилл группа 222
# ГО БДЗ 2

Это была очень тяжёлая домашка, в которой я прошло через все стадии пока трансформер не заработал: отрицание, гнев, торг, депрессия и принятие.

## Чекпоинт

Сначала я хотел написать быстренько RNN, но где-то набагал и не мог получить не нулевое Bleu, поэтому имея какое-то время в запасе, решил реализовать трансформер.

Выбить чекпоинт удалось с такими параметрами. Слова, которые встречались не более 1 раза я заменял на \<unk\>.

Оптимайзер 
```python
optim.AdamW(model.parameters(), lr = 1e-4, weight_decay = 0.333)
```

Параметры трансфомера
```python
d_model = 512, nhead = 8,
num_encoder_layers = 3, num_decoder_layers = 3,
dim_feedforward = 2048, dropout = 0.1,
```

#### Как окажется дальше, я допустил около 5 багов в разных частях решения, что в итоге отдебажил только 11 марта.

Тут и далее, чтобы экономить время, я обучал далеко не на всех данных, вот таким наивным образом ограничивая выборки.

```python
train_src = read_file(train_de_path)[:80000]
train_trg = read_file(train_en_path)[:80000]
valid_src = read_file(valid_de_path)[:200]
valid_trg = read_file(valid_en_path)[:200]
test_src  = read_file(test_de_path)
```


## Мысли про валидацию

В валидации порядка тысячи предложений. Поэтому я решил не добавлять и не смешивать это с обучающей выборкой, потому что это незначительная часть была бы. Кроме того, в ходе всех экспериментов я засылал модель и промежуточные результаты в бота. Получалось, что самая последняя версия всегда лучше, что можно было пронаблюдать на loss для обучающей выборки. Подсчёт loss для валидации мог бы помочь понять, есть ли переобучение например, но из слов выше, в этом не было острой необходимости, поэтому я решил отказаться от этого. Итого, я не использую данные из валидации.

## Мысли про Bleu

Я попытался вычислять Bleu, чтобы сравнивать модели. Как и в предыдущих мыслях получалось, что он просто монотонно рос, но ещё и очень долго вычислялся. Поэтому почти с самого начала я решил, что он мне особо не поможет и ограничился только вычислением loss на train выборке. В итоге, я засылал много промежуточных чекпоинтов обучения и решение оказалось оправданным. Приведу табличку, с количеством эпох в обучении и Bleu в боте у финальной модели.

0 эпох | 10 эпох | 20 эпох | 30 эпох | 40 эпох | 50 эпох(конец обучения)
--- | --- | --- | --- | --- | ---
0.0 | 19.38 | 21.57 | 24.66 | 27.22 | 27.72

## Подбор параметров.

Здесь результаты варьируются от 5.8 до 8, потому что были ошибки в модели, но найденные достаточно оптимальные параметры будут использоваться в дальнейшем.

* min_freq (если слово встречается в предложениях обучающей выборки на английском/немецком меньше min_freq раз, оно заменялось на \<unk\>)
  Попробовал варианты 1, 2, 7, 10, 12. Чем больше этот параметр, тем меньше размер алфавита и быстрее обучение (да и на самом деле получше в плане метрик). У меня был общий этот параметр, как для английского словаря, так и для немецкого, что я потом в итоге понимаю на 2 параметра. Значения 7-12 показали себя лучше всего в плане соотношения время обучения - результат, брать все слова очень долго учится, а если брать min_freq больше 12, то получаем достаточно маленький словарь слов на каждом из языков, что не приведёт к хорошему качеству.
* MAXLEN (max_len) - длина, к которой я привожу каждое предложение, дописывая \<pad\> в конец. Тут я ориентировался на длину предложений из выборок и выбрал число 40. В итоговой модели взял 80, потому что некоторые предложения были чуть подлиннее на глаз.
* У модели дефолтные

